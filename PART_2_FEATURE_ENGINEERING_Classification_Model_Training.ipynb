{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification Model Training: Job_Postings (PART 2-FEATURE ENGINEERING).ipynb",
      "provenance": [],
      "mount_file_id": "1MHGU74en_rOxhsl-JaxG0wQJ-gQRK3qv",
      "authorship_tag": "ABX9TyPKylxfw9Maya2Xv5yMdb5W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MashaKubyshina/Learning_to_code/blob/master/Classification_Model_Training_Job_Postings_(PART_2_FEATURE_ENGINEERING).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMmON3baQNf-"
      },
      "source": [
        "# The Feature Engineering is done following this path https://towardsdatascience.com/text-classification-in-python-dd95d264c802\n",
        "# To install it, please type this command in the shell(terminal): ! conda install -c conda-forge altair vega_datasets notebook vega"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQcduMSQeDJ"
      },
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "In this part we will transform the data in a set of inputs for our model\n",
        "Having good quality features helps us improve the model\n",
        "\n",
        "Dealing with text data there are several way to obtain features to represent the data\n",
        "\n",
        "\n",
        "\n",
        "1.   Text representation\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycSxvHAlS4Ei"
      },
      "source": [
        "**Text Cleaning and Preparation**\n",
        "\n",
        "There are varios feature creation methods\n",
        "These methods treat each row in the dataset as a separate document. These both methods are often named \"Bag of Words\" methods. They don't take into account the order of words.\n",
        "\n",
        "1.   Count word vectors\n",
        "2.   TF–IDF Vectors\n",
        "\n",
        "\n",
        "\"TF-IDF is a score that represents the relative importance of a term in the document and the entire corpus. TF stands for Term Frequency, and IDF stands for Inverse Document Frequency.\" from https://towardsdatascience.com/text-classification-in-python-dd95d264c802\n",
        "\n",
        "Other, more sophisticated methods are:\n",
        "\n",
        "\n",
        "1.   Word Embeddings\n",
        "2.   Text based or NLP based features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WHJwidUQnbJ"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "duFC-kgpQsaw",
        "outputId": "33e90da5-d592-4228-cf76-acb472b5fdab"
      },
      "source": [
        "# Open the dataset from https://data.world/jobspikr/10000-data-scientist-job-postings-from-the-usa\n",
        "# This dataset has 10000 entries of job postings in data science; the two columns we are particulary interested in are:\n",
        "# category and job_description\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://query.data.world/s/hpv4ejstsxjszdo2eathrfgjhzqlqy')\n",
        "df.head(10)"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crawl_timestamp</th>\n",
              "      <th>url</th>\n",
              "      <th>job_title</th>\n",
              "      <th>category</th>\n",
              "      <th>company_name</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "      <th>inferred_city</th>\n",
              "      <th>inferred_state</th>\n",
              "      <th>inferred_country</th>\n",
              "      <th>post_date</th>\n",
              "      <th>job_description</th>\n",
              "      <th>job_type</th>\n",
              "      <th>salary_offered</th>\n",
              "      <th>job_board</th>\n",
              "      <th>geo</th>\n",
              "      <th>cursor</th>\n",
              "      <th>contact_email</th>\n",
              "      <th>contact_phone_number</th>\n",
              "      <th>uniq_id</th>\n",
              "      <th>html_job_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-02-06 05:26:22 +0000</td>\n",
              "      <td>https://www.indeed.com/viewjob?jk=fd83355c2b23...</td>\n",
              "      <td>Enterprise Data Scientist I</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>Farmers Insurance Group</td>\n",
              "      <td>Woodland Hills</td>\n",
              "      <td>CA</td>\n",
              "      <td>Usa</td>\n",
              "      <td>Woodland hills</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>indeed</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549432819114777</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3b6c6acfcba6135a31c83bd7ea493b18</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-02-06 05:33:41 +0000</td>\n",
              "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Luxoft USA Inc</td>\n",
              "      <td>Middletown</td>\n",
              "      <td>NJ</td>\n",
              "      <td>Usa</td>\n",
              "      <td>Middletown</td>\n",
              "      <td>New jersey</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>We have an immediate opening for a Sharp Data ...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dice</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549432819122106</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>741727428839ae7ada852eebef29b0fe</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-02-06 05:33:35 +0000</td>\n",
              "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cincinnati Bell Technology Solutions</td>\n",
              "      <td>New York</td>\n",
              "      <td>NY</td>\n",
              "      <td>Usa</td>\n",
              "      <td>New york</td>\n",
              "      <td>New york</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Candidates should have the following backgroun...</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dice</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549432819236156</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cdc9ef9a1de327ccdc19cc0d07dbbb37</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-02-06 05:33:42 +0000</td>\n",
              "      <td>https://www.indeed.com/viewjob?jk=841edd86ead2...</td>\n",
              "      <td>Data Scientist, Aladdin Wealth Tech, Associate...</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>BlackRock</td>\n",
              "      <td>New York</td>\n",
              "      <td>NY 10055 (Midtown area)</td>\n",
              "      <td>Usa</td>\n",
              "      <td>New york</td>\n",
              "      <td>New york</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>indeed</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549432819259473</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1c8541cd2c2c924f9391c7d3f526f64e</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-02-06 05:48:23 +0000</td>\n",
              "      <td>https://job-openings.monster.com/senior-data-s...</td>\n",
              "      <td>Senior Data Scientist</td>\n",
              "      <td>biotech</td>\n",
              "      <td>CyberCoders</td>\n",
              "      <td>Charlotte</td>\n",
              "      <td>NC</td>\n",
              "      <td>Usa</td>\n",
              "      <td>Charlotte</td>\n",
              "      <td>North carolina</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>We are seeking an extraordinary Data Scientist...</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>NaN</td>\n",
              "      <td>monster</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549436429015957</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>445652a560a5441060857853cf267470</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2019-02-06 05:36:06 +0000</td>\n",
              "      <td>https://www.indeed.com/viewjob?jk=14fa3772cd5f...</td>\n",
              "      <td>CIB – Fixed Income Research – Machine Learning...</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>JP Morgan Chase</td>\n",
              "      <td>New York</td>\n",
              "      <td>NY 10179 (Midtown area)</td>\n",
              "      <td>Usa</td>\n",
              "      <td>New york</td>\n",
              "      <td>New york</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>indeed</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549436429033307</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9571ec617ba209fd9a4f842973a4e9c8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2019-02-06 05:34:37 +0000</td>\n",
              "      <td>https://www.indeed.com/viewjob?jk=c6db96b37f8a...</td>\n",
              "      <td>Data Scientist, Licensing Operations</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>Spotify</td>\n",
              "      <td>New York</td>\n",
              "      <td>NY 10011 (Chelsea area)</td>\n",
              "      <td>Usa</td>\n",
              "      <td>New york</td>\n",
              "      <td>New york</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>indeed</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549436429042348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0ec629c03f3e82651711f2626c23cadb</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2019-02-06 05:52:12 +0000</td>\n",
              "      <td>https://www.dice.com/jobs/detail/Sr.-Data-Scie...</td>\n",
              "      <td>Sr. Data Scientist (Can work on Xoriant W2)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Xoriant Corporation</td>\n",
              "      <td>Santa Clara</td>\n",
              "      <td>CA</td>\n",
              "      <td>Usa</td>\n",
              "      <td>Santa clara</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Job Title: - Sr. Data Science Consultant Durat...</td>\n",
              "      <td>Contract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dice</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549436429042523</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>972e897473d65f34b8e7f1c1b4c74b1c</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2019-02-06 05:34:18 +0000</td>\n",
              "      <td>https://www.indeed.com/viewjob?jk=0fc298b9f3a8...</td>\n",
              "      <td>Data Scientist, Aladdin Wealth Tech, Associate</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>BlackRock</td>\n",
              "      <td>New York</td>\n",
              "      <td>NY 10055 (Midtown area)</td>\n",
              "      <td>Usa</td>\n",
              "      <td>New york</td>\n",
              "      <td>New york</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>indeed</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549436429066810</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>80d64b46bc7c89602f63daf06b9f1b4c</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019-02-06 06:03:55 +0000</td>\n",
              "      <td>https://www.dice.com/jobs/detail/Data-Scientis...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Adroit Resources</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>CA</td>\n",
              "      <td>Usa</td>\n",
              "      <td>San francisco</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>• 3+ years related a professional experience  ...</td>\n",
              "      <td>Contract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dice</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549436429884667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b772c6ef8ee7631895ab9a59b5e8b2c1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             crawl_timestamp  ... html_job_description\n",
              "0  2019-02-06 05:26:22 +0000  ...                  NaN\n",
              "1  2019-02-06 05:33:41 +0000  ...                  NaN\n",
              "2  2019-02-06 05:33:35 +0000  ...                  NaN\n",
              "3  2019-02-06 05:33:42 +0000  ...                  NaN\n",
              "4  2019-02-06 05:48:23 +0000  ...                  NaN\n",
              "5  2019-02-06 05:36:06 +0000  ...                  NaN\n",
              "6  2019-02-06 05:34:37 +0000  ...                  NaN\n",
              "7  2019-02-06 05:52:12 +0000  ...                  NaN\n",
              "8  2019-02-06 05:34:18 +0000  ...                  NaN\n",
              "9  2019-02-06 06:03:55 +0000  ...                  NaN\n",
              "\n",
              "[10 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "VYfZLOVuQ26N",
        "outputId": "d911f819-55b2-4dc4-eeb6-8b94a4945d76"
      },
      "source": [
        "# Visualize 1 sample job description\n",
        "\n",
        "df.loc[1]['job_description']"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We have an immediate opening for a Sharp Data Scientist with a strong Mathematical/Statistical background to work on various initiatives in a Huge data environment. Will be looking at various formats of Data dealing with Billing Fraud Patterns using various software and modeling techniques. The candidate should have a minimum of 3 years of solid work experience in a professional organization/corporation using Python, SQL, Python libraries, Machine Learning Algorithms ( i.e. Decision Tree, Random Forest, Logistic Regression, etc.) should have strong Python skills. Additional experience with R Cloud would be a plus. Will be working with structured and unstructured data formats so any experience with Hive, and other Big Data technologies would be a plus. Should have strong communication skills and the ability to explain data findings to both Technical and Non- Technical audiences. We are NOT considering Recent College Grads- MUST have a minimum of 2-3 post Graduate experience working in a professional organization .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqLPg6jpoY4-"
      },
      "source": [
        "# special characters cleaning\n",
        "# \\r and \\n\n",
        "\n",
        "df['Content_Parsed_1'] = df['job_description'].str.replace(\"\\r\", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UFp1mFTo1dy"
      },
      "source": [
        "# \" when quoting text\n",
        "\n",
        "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Vaqfauo4NK"
      },
      "source": [
        "# Lowercasing the text\n",
        "\n",
        "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B9GQU51o83Y"
      },
      "source": [
        "#punctuation doesn't have any prediction, thus we will clean it\n",
        "\n",
        "punctuation_signs = list(\"?:!.,;\")\n",
        "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
        "\n",
        "for punct_sign in punctuation_signs:\n",
        "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjFO3zdIpGOS"
      },
      "source": [
        "# removing possesive pronouns termination\n",
        "\n",
        "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW-FTLAjpQLk",
        "outputId": "7ec8d471-f31d-4b1d-bc74-763500ebae77"
      },
      "source": [
        "# Stemming and Lemmatization\n",
        "# Since stemming can produce output words that don't exist, we'll only use a lemmatization process at this moment. \n",
        "# Lemmatization takes into consideration the morphological analysis of the words and returns words that do exist, \n",
        "# so it will be more useful for us.\n",
        "\n",
        "# Downloading punkt and wordnet from NLTK\n",
        "nltk.download('punkt')\n",
        "print(\"------------------------------------------------------------\")\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "------------------------------------------------------------\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZyeDhUHpuOk"
      },
      "source": [
        "# Saving the lemmatizer into an object\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIcv49ZWpxax"
      },
      "source": [
        "nrows = len(df)\n",
        "lemmatized_text_list = []\n",
        "\n",
        "for row in range(0, nrows):\n",
        "    \n",
        "    # Create an empty list containing lemmatized words\n",
        "    lemmatized_list = []\n",
        "    \n",
        "    # Save the text and its words into an object\n",
        "    text = df.loc[row]['Content_Parsed_4']\n",
        "    text_words = text.split(\" \")\n",
        "\n",
        "    # Iterate through every word to lemmatize\n",
        "    for word in text_words:\n",
        "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "        \n",
        "    # Join the list\n",
        "    lemmatized_text = \" \".join(lemmatized_list)\n",
        "    \n",
        "    # Append to the list containing the texts\n",
        "    lemmatized_text_list.append(lemmatized_text)"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCwNqk1Rp32o"
      },
      "source": [
        "df['Content_Parsed_5'] = lemmatized_text_list"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ehnsJdwqF9R",
        "outputId": "6e4c1a76-bc80-4b17-ffc5-31c6e5bd00cd"
      },
      "source": [
        "# Downloading the stop words list\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIdqS3pUqKTH"
      },
      "source": [
        "# Loading the stop words in english\n",
        "\n",
        "stop_words = list(stopwords.words('english'))"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNY_6BK_qNOE",
        "outputId": "0943644b-3648-431c-dc58-3e907202f398"
      },
      "source": [
        "# checking stopwords\n",
        "\n",
        "stop_words[0:10]"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU63r2jbqZdh"
      },
      "source": [
        "# we will use regular expressions to delete whole words\n",
        "\n",
        "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
        "\n",
        "for stop_word in stop_words:\n",
        "\n",
        "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "4r9HhsSMqoCe",
        "outputId": "6ccf27be-393b-4f1b-f26f-37a72134931d"
      },
      "source": [
        "# let's check the difference between original text and parsed text\n",
        "# original text\n",
        "\n",
        "df.loc[5]['job_description']"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Read what people are saying about working here. \\n\\nOpportunity\\n\\nThe opportunity is to join our New York team as an analyst, associate, or VP, with a focus on applications of machine learning and artificial intelligence in fixed income/securitized products markets. The role requires an individual with excellent technical and programming skills, preferably with an advanced degree in quantitative field (e.g., physics, computer science, mathematics, engineering).\\n\\nOur business\\n\\nJ.P. Morgan has the leading Global Rates and Global Spread business in terms of volume traded, issuers traded and quality of investor relationships. The Rates business covers Treasuries, swaps, options and other derivatives, while Spreads covers Credit, SPG, and Public Finance Markets. J.P. Morgan Global Spread and Trading offers first-class, highly integrated financial services to a global client base and provides financial assets and liquidity for banks, insurance companies, finance companies, mutual funds and hedge funds. Traders, salespeople and research analysts work collectively to generate ideas. The Credit business make secondary markets in high grade bonds/CDS, high yield bonds/CDS, distressed bonds, indices, options, correlation products, and more exotic structures. The Securitized Products Group (“SPG”) engages in origination, syndicate, sales & trading, financing, and principal investments activities. Asset classes include: mortgage-backed securities (commercial, residential, agency and non-agency), mortgage loans, consumer asset-backed securities and receivables (auto, credit card, student, equipment loans).\\n\\nOur team\\n\\nThe Fixed Income Strategy Team seeks to deliver best in class client-facing research across rates, structured products, municipals and short-term fixed income. We support risk takers and other groups across the CIB, CIO, as well as a wide range of external clients including the world’s leading hedge funds, asset managers, insurance companies, pensions, central banks, and others. We also work closely with partners in technology to incorporate the latest computational and data analysis techniques. The opportunity is to join our SPG research group as a junior data scientist/quantitative modeller.\\n\\nKey responsibilities could include:\\n\\nResponsible for applying machine learning/artificial intelligence and big data techniques to a variety of problems in securitized products research, including generating trading ideas and signals, improving prepayment/defaults forecasts for internal and external clients, and improving existing empirical techniques used in SPG research.\\n\\nTo begin areas of focus will be agency MBS, help automate various analysis using python, doing ad-hoc prepayment analysis for clients, and doing research of combining different machine learning techniques for prepayment modelling.\\n\\nDocumenting and communicating results for publication.\\n\\nThe individual will need to demonstrate a record of being a team player with potential to partner with teams of diverse functions: technology partners, control groups, strategists/modellers, and trading & sales\\n\\nThe ideal candidate has …\\n\\nAn advanced degree in physics, engineering, math, statistics, computer science or other quantitative fields\\n\\nExceptional analytical, quantitative and problem-solving skills\\n\\nStrong programming skills (preferably with Python), and some C++ knowledge and experience\\n\\nQuick learner and self-starter, has strong interests to learn new developments in machine learning and artificial intelligence\\n\\nExcellent written and oral communication and interpersonal skills, and an ability to work in a fast-paced, highly collaborative environment\\n\\nExperience with and/or a strong interest in financial markets and macroeconomics, and their impacts to consumer behaviours.\\n\\nKnowledge of fixed income markets, especially in MBS and models, is a plus, but is not a strict requirement.\\n\\n At JPMorgan Chase, the work we do matters. All of us are committed to putting our resources and our voices to work every day for our cust...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "CNlklEv2q0Op",
        "outputId": "feab8d80-52c4-48b8-f345-546c46d83f4f"
      },
      "source": [
        "# parsed text\n",
        "\n",
        "df.loc[5]['Content_Parsed_6']"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'read  people  say  work    opportunity   opportunity   join  new york team   analyst associate  vp   focus  applications  machine learn  artificial intelligence  fix income/securitized products market  role require  individual  excellent technical  program skills preferably   advance degree  quantitative field (eg physics computer science mathematics engineering)   business  jp morgan   lead global rat  global spread business  term  volume trade issuers trade  quality  investor relationships  rat business cover treasuries swap options   derivatives  spread cover credit spg  public finance market jp morgan global spread  trade offer first-class highly integrate financial service   global client base  provide financial assets  liquidity  bank insurance company finance company mutual fund  hedge fund traders salespeople  research analysts work collectively  generate ideas  credit business make secondary market  high grade bonds/cds high yield bonds/cds distress bond indices options correlation products   exotic structure  securitized products group (“spg”) engage  origination syndicate sales & trade finance  principal investments activities asset class include mortgage-backed securities (commercial residential agency  non-agency) mortgage loan consumer asset-backed securities  receivables (auto credit card student equipment loans)   team   fix income strategy team seek  deliver best  class client-facing research across rat structure products municipals  short-term fix income  support risk takers   group across  cib cio  well   wide range  external clients include  world’ lead hedge fund asset managers insurance company pension central bank  others  also work closely  partner  technology  incorporate  latest computational  data analysis techniques  opportunity   join  spg research group   junior data scientist/quantitative modeller  key responsibilities could include  responsible  apply machine learning/artificial intelligence  big data techniques   variety  problems  securitized products research include generate trade ideas  signal improve prepayment/defaults forecast  internal  external clients  improve exist empirical techniques use  spg research   begin areas  focus   agency mbs help automate various analysis use python  ad-hoc prepayment analysis  clients   research  combine different machine learn techniques  prepayment model  document  communicate result  publication   individual  need  demonstrate  record    team player  potential  partner  team  diverse function technology partner control group strategists/modellers  trade & sales   ideal candidate  …   advance degree  physics engineer math statistics computer science   quantitative field  exceptional analytical quantitative  problem-solving skills  strong program skills (preferably  python)   c++ knowledge  experience  quick learner  self-starter  strong interest  learn new developments  machine learn  artificial intelligence  excellent write  oral communication  interpersonal skills   ability  work   fast-paced highly collaborative environment  experience  /  strong interest  financial market  macroeconomics   impact  consumer behaviours  knowledge  fix income market especially  mbs  model   plus     strict requirement    jpmorgan chase  work   matter   us  commit  put  resources   voice  work every day   cust'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "ud01h4AbrBwJ",
        "outputId": "6821b166-abd1-4909-f4d5-5700d594dbfa"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crawl_timestamp</th>\n",
              "      <th>url</th>\n",
              "      <th>job_title</th>\n",
              "      <th>category</th>\n",
              "      <th>company_name</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "      <th>inferred_city</th>\n",
              "      <th>inferred_state</th>\n",
              "      <th>inferred_country</th>\n",
              "      <th>post_date</th>\n",
              "      <th>job_description</th>\n",
              "      <th>job_type</th>\n",
              "      <th>salary_offered</th>\n",
              "      <th>job_board</th>\n",
              "      <th>geo</th>\n",
              "      <th>cursor</th>\n",
              "      <th>contact_email</th>\n",
              "      <th>contact_phone_number</th>\n",
              "      <th>uniq_id</th>\n",
              "      <th>html_job_description</th>\n",
              "      <th>Content_Parsed_1</th>\n",
              "      <th>Content_Parsed_2</th>\n",
              "      <th>Content_Parsed_3</th>\n",
              "      <th>Content_Parsed_4</th>\n",
              "      <th>Content_Parsed_5</th>\n",
              "      <th>Content_Parsed_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-02-06 05:26:22 +0000</td>\n",
              "      <td>https://www.indeed.com/viewjob?jk=fd83355c2b23...</td>\n",
              "      <td>Enterprise Data Scientist I</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>Farmers Insurance Group</td>\n",
              "      <td>Woodland Hills</td>\n",
              "      <td>CA</td>\n",
              "      <td>Usa</td>\n",
              "      <td>Woodland hills</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>NaN</td>\n",
              "      <td>indeed</td>\n",
              "      <td>usa</td>\n",
              "      <td>1549432819114777</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3b6c6acfcba6135a31c83bd7ea493b18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read what people are saying about working here...</td>\n",
              "      <td>read what people are saying about working here...</td>\n",
              "      <td>read what people are saying about working here...</td>\n",
              "      <td>read what people be say about work here   we b...</td>\n",
              "      <td>read  people  say  work      farmers  join  te...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             crawl_timestamp  ...                                   Content_Parsed_6\n",
              "0  2019-02-06 05:26:22 +0000  ...  read  people  say  work      farmers  join  te...\n",
              "\n",
              "[1 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbAUzNe-rJ45"
      },
      "source": [
        "# we will rearrange the dataset and only use the columns we need\n",
        "\n",
        "list_columns = ['uniq_id','job_title', 'category', 'company_name', 'state', 'post_date','job_type', 'job_description', 'Content_Parsed_6']\n",
        "df = df[list_columns]\n",
        "\n",
        "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "TfRhkCCFsDWY",
        "outputId": "118d982c-e47a-4156-bda1-49e6164549b8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniq_id</th>\n",
              "      <th>job_title</th>\n",
              "      <th>category</th>\n",
              "      <th>company_name</th>\n",
              "      <th>state</th>\n",
              "      <th>post_date</th>\n",
              "      <th>job_type</th>\n",
              "      <th>job_description</th>\n",
              "      <th>Content_Parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3b6c6acfcba6135a31c83bd7ea493b18</td>\n",
              "      <td>Enterprise Data Scientist I</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>Farmers Insurance Group</td>\n",
              "      <td>CA</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work      farmers  join  te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>741727428839ae7ada852eebef29b0fe</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Luxoft USA Inc</td>\n",
              "      <td>NJ</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>We have an immediate opening for a Sharp Data ...</td>\n",
              "      <td>immediate open   sharp data scientist   str...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cdc9ef9a1de327ccdc19cc0d07dbbb37</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cincinnati Bell Technology Solutions</td>\n",
              "      <td>NY</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>Candidates should have the following backgroun...</td>\n",
              "      <td>candidates    follow background skills  charac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1c8541cd2c2c924f9391c7d3f526f64e</td>\n",
              "      <td>Data Scientist, Aladdin Wealth Tech, Associate...</td>\n",
              "      <td>Accounting/Finance</td>\n",
              "      <td>BlackRock</td>\n",
              "      <td>NY 10055 (Midtown area)</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work     blackrock  blackro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>445652a560a5441060857853cf267470</td>\n",
              "      <td>Senior Data Scientist</td>\n",
              "      <td>biotech</td>\n",
              "      <td>CyberCoders</td>\n",
              "      <td>NC</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>We are seeking an extraordinary Data Scientist...</td>\n",
              "      <td>seek  extraordinary data scientist  charlott...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            uniq_id  ...                                     Content_Parsed\n",
              "0  3b6c6acfcba6135a31c83bd7ea493b18  ...  read  people  say  work      farmers  join  te...\n",
              "1  741727428839ae7ada852eebef29b0fe  ...     immediate open   sharp data scientist   str...\n",
              "2  cdc9ef9a1de327ccdc19cc0d07dbbb37  ...  candidates    follow background skills  charac...\n",
              "3  1c8541cd2c2c924f9391c7d3f526f64e  ...  read  people  say  work     blackrock  blackro...\n",
              "4  445652a560a5441060857853cf267470  ...    seek  extraordinary data scientist  charlott...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yJaXEEDtGXY"
      },
      "source": [
        "**Subsetting the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UMLVtSmuv27",
        "outputId": "ff5dbe48-6149-456f-8475-a11a0b8088ce"
      },
      "source": [
        "# we will create a dictionary with the labels for categories (this is where our previous work of categorizing will come handy)\n",
        "\n",
        "# For this project I will work with the data from 3 categories that are balanced, I will repeat her ethe code from EDA-Part 1\n",
        "\n",
        "\n",
        "df=df.dropna(how='any',axis=0)\n",
        "\n",
        "df.shape"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8702, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS8HcnRytNc_",
        "outputId": "478f906b-eea3-4b49-fa17-037c6b01a1e6"
      },
      "source": [
        "# splitting the long names into words\n",
        "\n",
        "df['category']=df['category'].str.split().str[0]\n",
        "df['category'].unique()"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Accounting/Finance', 'biotech', 'Computer/Internet',\n",
              "       'Arts/Entertainment/Publishing', 'military', 'business',\n",
              "       'Engineering/Architecture', 'Manufacturing/Mechanical', 'life',\n",
              "       'Banking/Loans', 'agriculture', 'Education/Training', 'science',\n",
              "       'arts', 'Customer+Service', 'food', 'technology', 'Healthcare',\n",
              "       'Government/Military', 'architecture', 'community', 'production',\n",
              "       'r', 'Insurance', 'security', 'healthcare', 'Human+Resources',\n",
              "       'Administrative', 'language', 'media', 'Sales', 'advertising',\n",
              "       'office', 'Restaurant/Food+Service', 'Telecommunications',\n",
              "       'Construction/Facilities', 'Legal', 'Marketing/Advertising/PR',\n",
              "       'Non-Profit/Volunteering', 'Pharmaceutical/Bio-tech', 'education',\n",
              "       'management', 'transportation', 'protective', 'legal', 'sales',\n",
              "       'Retail', 'Hospitality/Travel', 'building',\n",
              "       'Transportation/Logistics', 'math', 'Upper+Management/Consulting',\n",
              "       'Accounting-or-finance', 'Computer-or-internet',\n",
              "       'Manufacturing-or-mechanical', 'Engineering-or-architecture',\n",
              "       'Government-or-military', 'Human-Resources',\n",
              "       'Construction-or-facilities', 'Banking-or-loans',\n",
              "       'Transportation-or-logistics',\n",
              "       'Arts-or-entertainment-or-publishing',\n",
              "       'Law-Enforcement-or-security', 'Marketing-or-advertising-or-pr',\n",
              "       'computer', 'service', 'Customer-Service', 'social',\n",
              "       'Restaurant-or-food-Service', 'finance', 'administrative',\n",
              "       'Education-or-training', 'engineering', 'entertainment',\n",
              "       'construction', 'manufacturing', 'financieel', 'animal', 'higher',\n",
              "       'Hospitality-or-travel', 'personal', 'Non-profit/volunteering',\n",
              "       'Upper-Management-or-consulting', 'banking',\n",
              "       'Pharmaceutical/bio-tech', 'Science', 'communications', 'Admin',\n",
              "       'executive', 'Data', 'Senior', 'TV', 'Principal', 'Vice', 'Sr.',\n",
              "       'Applied', 'Digital', 'DISTINGUISHED', 'Manager,', 'Lead',\n",
              "       'Quality', 'Mid', 'Sr', 'Agile', 'Finance', 'Intel', 'Staff',\n",
              "       'Manufacturing', 'Clinical', 'Customer', 'Systems,', 'Geospatial',\n",
              "       'Mgr/Sr', 'Deep', 'Marketing', 'Optimization', 'AI', 'Research',\n",
              "       'Revenue', 'Associate', 'Full-time', 'Actuarial', 'Homeland',\n",
              "       'Distinguished', 'Director', 'Federal', 'Junior', 'Operational',\n",
              "       'Genomics', 'Assistant', 'Director,', 'Network', 'Software', 'New',\n",
              "       'Managing', 'Advanced', 'Soil', 'Analytics,', 'Mid-Level', 'VP',\n",
              "       'Brand', 'Machine', 'Precision', 'Statistical', 'Founding'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fir-GuI7t0dI"
      },
      "source": [
        "# I will solidify categories\n",
        "\n",
        "df['category']= df['category'].replace(['Accounting/Finance','Accounting-or-finance','finance','financieel','Banking','Banking/Loans','Banking-or-loans'],'Finance')\n",
        "df['category']= df['category'].replace(['biotech','Pharmaceutical/Bio-tech','science','math'],'Biotech')\n",
        "df['category']= df['category'].replace(['Computer/Internet','technology','Computer-or-internet','computer'],'Computer/Tech')\n",
        "df['category']= df['category'].replace(['life','social','r','animal','higher','personal'],'Misc')\n",
        "df['category']= df['category'].replace(['Arts/Entertainment/Publishing','Arts-or-entertainment-or-publishing','entertainment','Entertainment','production','media','community','advertising','Marketing/Advertising/PR','Sales','sales','Marketing-or-advertising-or-pr','SocialMedia/Marketing'],'Marketing/Ads/Entertainment')\n",
        "df['category']= df['category'].replace(['military','Government/Military','science','Government-or-military'],'Military')\n",
        "df['category']= df['category'].replace(['business','management','Upper-Management-or-consulting'],'Business/Consulting')\n",
        "df['category']= df['category'].replace(['Administrative','office','administrative','Human+Resources','Human-Resources','Customer+Service','Telecommunications','service','Customer-Service','CustomerService'],'Administrative/HR')\n",
        "df['category']= df['category'].replace(['Engineering/Architecture','architecture','Engineering-or-architecture','engineering'],'Engineering/Architecture')\n",
        "df['category']= df['category'].replace(['Manufacturing/Mechanical','production','manufacturing','Manufacturing-or-mechanical'],'Manufacturing')\n",
        "df['category']= df['category'].replace(['agriculture',],'Agriculture')\n",
        "df['category']= df['category'].replace(['Restaurant/Food+Service','food','Restaurant-or-food-Service'],'FoodService')\n",
        "df['category']= df['category'].replace(['Education/Training','education','Education-or-training'],'Education/Training')\n",
        "df['category']= df['category'].replace(['science','math'],'Math/Science')\n",
        "df['category']= df['category'].replace(['Healthcare','healthcare'],'Healthcare')\n",
        "df['category']= df['category'].replace(['Insurance','protective'],'Insurance')\n",
        "df['category']= df['category'].replace(['security','Law-Enforcement-or-security'],'Security')\n",
        "df['category']= df['category'].replace(['language','arts','Arts'],'Language/Arts')\n",
        "df['category']= df['category'].replace(['Construction/Facilities','building','Construction-or-facilities','construction'],'Construction')\n",
        "df['category']= df['category'].replace(['Legal','legal'],'Legal')\n",
        "df['category']= df['category'].replace(['transportation','Transportation/Logistics','Transportation-or-logistics'],'Transportation')\n",
        "df['category']= df['category'].replace(['Hospitality-or-travel','Hospitality/Travel'],'Hospitality/Travel')"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbnxOg4YuSGY",
        "outputId": "b3f2a529-d7e0-45d7-cb21-dc45c44f078a"
      },
      "source": [
        "df['category'].unique()"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Finance', 'Biotech', 'Computer/Tech',\n",
              "       'Marketing/Ads/Entertainment', 'Military', 'Business/Consulting',\n",
              "       'Engineering/Architecture', 'Manufacturing', 'Misc', 'Agriculture',\n",
              "       'Education/Training', 'Language/Arts', 'Administrative/HR',\n",
              "       'FoodService', 'Healthcare', 'Insurance', 'Security',\n",
              "       'Construction', 'Legal', 'Non-Profit/Volunteering',\n",
              "       'Transportation', 'Retail', 'Hospitality/Travel',\n",
              "       'Upper+Management/Consulting', 'Non-profit/volunteering',\n",
              "       'banking', 'Pharmaceutical/bio-tech', 'Science', 'communications',\n",
              "       'Admin', 'executive', 'Data', 'Senior', 'TV', 'Principal', 'Vice',\n",
              "       'Sr.', 'Applied', 'Digital', 'DISTINGUISHED', 'Manager,', 'Lead',\n",
              "       'Quality', 'Mid', 'Sr', 'Agile', 'Intel', 'Staff', 'Clinical',\n",
              "       'Customer', 'Systems,', 'Geospatial', 'Mgr/Sr', 'Deep',\n",
              "       'Marketing', 'Optimization', 'AI', 'Research', 'Revenue',\n",
              "       'Associate', 'Full-time', 'Actuarial', 'Homeland', 'Distinguished',\n",
              "       'Director', 'Federal', 'Junior', 'Operational', 'Genomics',\n",
              "       'Assistant', 'Director,', 'Network', 'Software', 'New', 'Managing',\n",
              "       'Advanced', 'Soil', 'Analytics,', 'Mid-Level', 'VP', 'Brand',\n",
              "       'Machine', 'Precision', 'Statistical', 'Founding'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "vkI6Smh2vjS5",
        "outputId": "952297f8-3ebd-4a64-9b48-8f0a3a051f0a"
      },
      "source": [
        "# I will use only 3 categories here\n",
        "\n",
        "df=df[(df.category == 'Computer/Tech') | (df.category == 'Engineering/Architecture') | (df.category == 'Business/Consulting')]\n",
        "df.head(10)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniq_id</th>\n",
              "      <th>job_title</th>\n",
              "      <th>category</th>\n",
              "      <th>company_name</th>\n",
              "      <th>state</th>\n",
              "      <th>post_date</th>\n",
              "      <th>job_type</th>\n",
              "      <th>job_description</th>\n",
              "      <th>Content_Parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9cd3ed78e5cac9e516ea41173de2c25f</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Northrop Grumman</td>\n",
              "      <td>CA 93940</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work     northrop grumman i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>53224da901548e7137bbb163d456ba6a</td>\n",
              "      <td>ETL Developer / Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Noblis</td>\n",
              "      <td>VA 20191</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    responsibilities   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>010b5d671896d26eba50948c7c337f94</td>\n",
              "      <td>Research Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>ARUP Laboratories</td>\n",
              "      <td>UT</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    job detail  descrip...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4a8b875d1acc4f560716561d699aa022</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Bank of America</td>\n",
              "      <td>WA 98104 (First Hill area)</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    job description  po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>edcdf4dd38cca4bbccd5ba0b787d2c49</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Sallie Mae</td>\n",
              "      <td>DE</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work     operations strateg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2956d0023b0c4617c430adcedb1e38d9</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>VideoAmp</td>\n",
              "      <td>CA</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work     us  videoamp’ miss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>e457f78180e72dd1ddee0efc042b0496</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Apple</td>\n",
              "      <td>MA</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    summary  post feb 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>b6e361550638f071a4985bf5f3d440ce</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Business/Consulting</td>\n",
              "      <td>Apex Systems</td>\n",
              "      <td>OH</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>Description Building large systematic reports ...</td>\n",
              "      <td>description build large systematic report  one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>d33577ea9ae09c58d77e1fab2c012ba2</td>\n",
              "      <td>Senior Data Scientist - Tallahassee, FL - $150...</td>\n",
              "      <td>Business/Consulting</td>\n",
              "      <td>Jefferson Frank</td>\n",
              "      <td>FL</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>My client is a leader in the Manufacturing ver...</td>\n",
              "      <td>client   leader   manufacture vertical   oper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>09378ddde9b997b1acbf519c2b9ddf03</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Business/Consulting</td>\n",
              "      <td>5 Star Global Recruitment Partners, LLC</td>\n",
              "      <td>TX</td>\n",
              "      <td>2019-02-05</td>\n",
              "      <td>Full Time</td>\n",
              "      <td>Data Scientist Client: A Fortune 500, very lar...</td>\n",
              "      <td>data scientist client  fortune 500  large mult...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             uniq_id  ...                                     Content_Parsed\n",
              "10  9cd3ed78e5cac9e516ea41173de2c25f  ...  read  people  say  work     northrop grumman i...\n",
              "12  53224da901548e7137bbb163d456ba6a  ...  read  people  say  work    responsibilities   ...\n",
              "13  010b5d671896d26eba50948c7c337f94  ...  read  people  say  work    job detail  descrip...\n",
              "18  4a8b875d1acc4f560716561d699aa022  ...  read  people  say  work    job description  po...\n",
              "19  edcdf4dd38cca4bbccd5ba0b787d2c49  ...  read  people  say  work     operations strateg...\n",
              "21  2956d0023b0c4617c430adcedb1e38d9  ...  read  people  say  work     us  videoamp’ miss...\n",
              "22  e457f78180e72dd1ddee0efc042b0496  ...  read  people  say  work    summary  post feb 5...\n",
              "32  b6e361550638f071a4985bf5f3d440ce  ...  description build large systematic report  one...\n",
              "33  d33577ea9ae09c58d77e1fab2c012ba2  ...   client   leader   manufacture vertical   oper...\n",
              "34  09378ddde9b997b1acbf519c2b9ddf03  ...  data scientist client  fortune 500  large mult...\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J6wG0fgv3yR",
        "outputId": "8325ce9e-f1a6-4952-f8f8-2fe365b90c84"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4664, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA-_ZP1AQJwB"
      },
      "source": [
        "df=df.iloc[:2000]"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzMls3PFQMbS",
        "outputId": "dc3690b2-907e-4f5f-fded-dcb5f83bd00e"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elo2FLV_v5Jt",
        "outputId": "f8afb8a5-28d2-42ff-eff3-b06cb50eefe4"
      },
      "source": [
        "df.describe"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                                uniq_id  ...                                     Content_Parsed\n",
              "10    9cd3ed78e5cac9e516ea41173de2c25f  ...  read  people  say  work     northrop grumman i...\n",
              "12    53224da901548e7137bbb163d456ba6a  ...  read  people  say  work    responsibilities   ...\n",
              "13    010b5d671896d26eba50948c7c337f94  ...  read  people  say  work    job detail  descrip...\n",
              "18    4a8b875d1acc4f560716561d699aa022  ...  read  people  say  work    job description  po...\n",
              "19    edcdf4dd38cca4bbccd5ba0b787d2c49  ...  read  people  say  work     operations strateg...\n",
              "...                                ...  ...                                                ...\n",
              "4216  fae59097315afaf0dfc4c49327c29f55  ...   good people build reward career   think  work...\n",
              "4217  d54a1ab24d42b54b9d95f6d3116accac  ...  senior data scientist machine learn   thrive k...\n",
              "4218  2cfcbe7a850a5b9dabc04622602faa4f  ...   good people build reward career   think  work...\n",
              "4219  899bccb75fe20334a8e9dff5fe4ffaf4  ...   good people build reward career   think  work...\n",
              "4220  c87cf45fce78c0ebbd6a27da6168d04d  ...   click  “apply” button  understand   employmen...\n",
              "\n",
              "[2000 rows x 9 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzW2sTwSwMYv"
      },
      "source": [
        "**Label Coding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWMnTMI9wPxm"
      },
      "source": [
        "category_codes = {\n",
        "    'Computer/Tech': 0,\n",
        "    'Engineering/Architecture': 1,\n",
        "    'Business/Consulting': 2\n",
        "}"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAUxglidwcxW"
      },
      "source": [
        "# Category mapping\n",
        "\n",
        "df['Category_Code'] = df['category']\n",
        "df = df.replace({'Category_Code':category_codes})"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "BzQeIlBbwgtl",
        "outputId": "2bc9a7b9-1c9e-4b25-b9e6-6e74565845e2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniq_id</th>\n",
              "      <th>job_title</th>\n",
              "      <th>category</th>\n",
              "      <th>company_name</th>\n",
              "      <th>state</th>\n",
              "      <th>post_date</th>\n",
              "      <th>job_type</th>\n",
              "      <th>job_description</th>\n",
              "      <th>Content_Parsed</th>\n",
              "      <th>Category_Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9cd3ed78e5cac9e516ea41173de2c25f</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Northrop Grumman</td>\n",
              "      <td>CA 93940</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work     northrop grumman i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>53224da901548e7137bbb163d456ba6a</td>\n",
              "      <td>ETL Developer / Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Noblis</td>\n",
              "      <td>VA 20191</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    responsibilities   ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>010b5d671896d26eba50948c7c337f94</td>\n",
              "      <td>Research Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>ARUP Laboratories</td>\n",
              "      <td>UT</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    job detail  descrip...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4a8b875d1acc4f560716561d699aa022</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Bank of America</td>\n",
              "      <td>WA 98104 (First Hill area)</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work    job description  po...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>edcdf4dd38cca4bbccd5ba0b787d2c49</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Computer/Tech</td>\n",
              "      <td>Sallie Mae</td>\n",
              "      <td>DE</td>\n",
              "      <td>2019-02-06</td>\n",
              "      <td>Undefined</td>\n",
              "      <td>Read what people are saying about working here...</td>\n",
              "      <td>read  people  say  work     operations strateg...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             uniq_id  ... Category_Code\n",
              "10  9cd3ed78e5cac9e516ea41173de2c25f  ...             0\n",
              "12  53224da901548e7137bbb163d456ba6a  ...             0\n",
              "13  010b5d671896d26eba50948c7c337f94  ...             0\n",
              "18  4a8b875d1acc4f560716561d699aa022  ...             0\n",
              "19  edcdf4dd38cca4bbccd5ba0b787d2c49  ...             0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcPcMsGDxLWT"
      },
      "source": [
        "**Train and Test Splits**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j32tUKRbwn9X"
      },
      "source": [
        "# Since we have a large number of entries, we are good using 25% for testing\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
        "                                                    df['Category_Code'], \n",
        "                                                    test_size=0.25, \n",
        "                                                    random_state=8)"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8VSqvSwxFo9"
      },
      "source": [
        "**Text Representation**\n",
        "\n",
        "We will use TF-IDF Vectors as features in this project.\n",
        "\n",
        "We have to define the different parameters:\n",
        "*  ngram_range: We want to consider both unigrams and bigrams.\n",
        "*  max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
        "*  min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "*  max_features: If not None, build a vocabulary that only consider the top *  max_features ordered by term frequency across the corpus.\n",
        "\n",
        "\n",
        "For more information refer to this tutorial https://github.com/miguelfzafra/Latest-News-Classifier/blob/master/0.%20Latest%20News%20Classifier/03.%20Feature%20Engineering/03.%20Feature%20Engineering.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCrPlMgpxEua"
      },
      "source": [
        "# Parameter election\n",
        "ngram_range = (1,2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 300"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGhwNiyax_k9",
        "outputId": "3839c4d1-c0a8-43c2-d375-9bbbecc2bea1"
      },
      "source": [
        "# We can try different combination of the params to improve accuracy of the models\n",
        "\n",
        "tfidf = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "                        \n",
        "features_train = tfidf.fit_transform(X_train).toarray()\n",
        "labels_train = y_train\n",
        "print(\"Train features\", features_train.shape)\n",
        "\n",
        "features_test = tfidf.transform(X_test).toarray()\n",
        "labels_test = y_test\n",
        "print(\"Test features\", features_test.shape)\n",
        "\n",
        "# we have fitted and then transformed the training set, but we have only transformed the test set.\n",
        "# for train we have: tfidf.fit_transform and for test: tfidf.transform"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train features (1500, 300)\n",
            "Test features (500, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRrR_zxfyy2S",
        "outputId": "c5a81dd9-04e2-448d-8200-76f6cdb541f4"
      },
      "source": [
        "# We can use the Chi squared test in order to see what unigrams and bigrams are most correlated with each category:\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "for Product, category_id in sorted(category_codes.items()):\n",
        "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
        "    indices = np.argsort(features_chi2[0])\n",
        "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "    print(\"# '{}' category:\".format(Product))\n",
        "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
        "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-3:])))\n",
        "    print(\"\")"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# 'Business/Consulting' category:\n",
            "  . Most correlated unigrams:\n",
            ". read\n",
            ". please\n",
            ". say\n",
            ". aws\n",
            ". client\n",
            "  . Most correlated bigrams:\n",
            ". read people\n",
            ". say work\n",
            ". people say\n",
            "\n",
            "# 'Computer/Tech' category:\n",
            "  . Most correlated unigrams:\n",
            ". code\n",
            ". computer\n",
            ". please\n",
            ". client\n",
            ". aws\n",
            "  . Most correlated bigrams:\n",
            ". say work\n",
            ". computer science\n",
            ". deep learn\n",
            "\n",
            "# 'Engineering/Architecture' category:\n",
            "  . Most correlated unigrams:\n",
            ". client\n",
            ". mission\n",
            ". write\n",
            ". power\n",
            ". security\n",
            "  . Most correlated bigrams:\n",
            ". say work\n",
            ". read people\n",
            ". people say\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqqxQQbWzA1V",
        "outputId": "610cac97-1266-498c-9a18-7aac719d8ba0"
      },
      "source": [
        "# we can see that bigrams and unigrams correspond well to the topic; bigrams correspond better\n",
        "# we can check the birgams\n",
        "\n",
        "[bigrams]"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sexual orientation',\n",
              "  'national origin',\n",
              "  'machine learn',\n",
              "  'veteran status',\n",
              "  'predictive model',\n",
              "  'data set',\n",
              "  'relate field',\n",
              "  'gender identity',\n",
              "  'data source',\n",
              "  'data science',\n",
              "  'experience work',\n",
              "  'equal opportunity',\n",
              "  'data mine',\n",
              "  'communication skills',\n",
              "  'data analysis',\n",
              "  'years experience',\n",
              "  'data analytics',\n",
              "  'opportunity employer',\n",
              "  'deep learn',\n",
              "  'computer science',\n",
              "  'data scientists',\n",
              "  'experience data',\n",
              "  'data scientist',\n",
              "  'bachelor degree',\n",
              "  'big data',\n",
              "  'say work',\n",
              "  'read people',\n",
              "  'people say']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1WFNwR7zu4q"
      },
      "source": [
        "# we will save the files we need for next steps\n",
        "# for this code to work you need to first create directory\n",
        "# I used google drive to be able to access these files elsewhere\n",
        "\n",
        "# X_train\n",
        "with open('/content/drive/MyDrive/Pickles/X_train.pickle', 'wb') as output:\n",
        "    pickle.dump(X_train, output)\n",
        "    \n",
        "# X_test    \n",
        "with open('/content/drive/MyDrive/Pickles/X_test.pickle', 'wb') as output:\n",
        "    pickle.dump(X_test, output)\n",
        "    \n",
        "# y_train\n",
        "with open('/content/drive/MyDrive/Pickles/y_train.pickle', 'wb') as output:\n",
        "    pickle.dump(y_train, output)\n",
        "    \n",
        "# y_test\n",
        "with open('/content/drive/MyDrive/Pickles/y_test.pickle', 'wb') as output:\n",
        "    pickle.dump(y_test, output)\n",
        "    \n",
        "# df\n",
        "with open('/content/drive/MyDrive/Pickles/df.pickle', 'wb') as output:\n",
        "    pickle.dump(df, output)\n",
        "    \n",
        "# features_train\n",
        "with open('/content/drive/MyDrive/Pickles/features_train.pickle', 'wb') as output:\n",
        "    pickle.dump(features_train, output)\n",
        "\n",
        "# labels_train\n",
        "with open('/content/drive/MyDrive/Pickles/labels_train.pickle', 'wb') as output:\n",
        "    pickle.dump(labels_train, output)\n",
        "\n",
        "# features_test\n",
        "with open('/content/drive/MyDrive/Pickles/features_test.pickle', 'wb') as output:\n",
        "    pickle.dump(features_test, output)\n",
        "\n",
        "# labels_test\n",
        "with open('/content/drive/MyDrive/Pickles/labels_test.pickle', 'wb') as output:\n",
        "    pickle.dump(labels_test, output)\n",
        "    \n",
        "# TF-IDF object\n",
        "with open('/content/drive/MyDrive/Pickles/tfidf.pickle', 'wb') as output:\n",
        "    pickle.dump(tfidf, output)"
      ],
      "execution_count": 310,
      "outputs": []
    }
  ]
}
